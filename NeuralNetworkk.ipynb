{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ac82eec1",
      "metadata": {
        "id": "ac82eec1"
      },
      "source": [
        "## Is Science Becoming Less Disruptive?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f315b84",
      "metadata": {
        "id": "9f315b84"
      },
      "source": [
        "* This is a project created in the context of the course: \"Applied Machine Learning\". <br>\n",
        "* In this assignment we will create a Machine Learning model that can predict, as accurately as possible, the CD5 of a published paper. <br>\n",
        "* The analysis will be done on Google Colab\n",
        "* This Jupyter in **continue**(**part b of assignemnt**) of Jupyter NonNeuralNetwork, it conatins an implemenation of Regression using Neural Network.\n",
        "\n",
        "---\n",
        ">Evangelia P. Panourgia, Data Scientist <br>\n",
        ">Id : t8190130 <br>\n",
        ">Department of Management Science and Technology <br>\n",
        ">Athens University of Economics and Business <br>\n",
        ">eva.panourgiaa@gmail.com <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df248753",
      "metadata": {
        "id": "df248753"
      },
      "source": [
        "## Neural Network Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To start with, we will load teh needeed libraries."
      ],
      "metadata": {
        "id": "xh6qkG0UFcEH"
      },
      "id": "xh6qkG0UFcEH"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import random as rn\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import layers\n",
        "# from tensorflow.keras.layers.experimental import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "jhxwsQ5utfUQ"
      },
      "id": "jhxwsQ5utfUQ",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We will read our data that we saved from our main jupyter.\n",
        "* You can take the original analyzed data from the folder `copy` and take from it the csv named :: `this_is_the_final_csv_for_google_colab.csv` and put this csv in Google Colab and you will have exactly the same reproduction of MAE and loss plot.     "
      ],
      "metadata": {
        "id": "_nu1hAbyr2f8"
      },
      "id": "_nu1hAbyr2f8"
    },
    {
      "cell_type": "code",
      "source": [
        "all = pd.read_csv('this_is_the_final_csv_for_google_colab.csv')"
      ],
      "metadata": {
        "id": "wmeHvyu3bwoK"
      },
      "id": "wmeHvyu3bwoK",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all.drop('Unnamed: 0',axis=1,inplace=True) # we will drom this useless column "
      ],
      "metadata": {
        "id": "bvZil77ybwvi"
      },
      "id": "bvZil77ybwvi",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's preview our dataframe."
      ],
      "metadata": {
        "id": "OZuy0Ju0F8nP"
      },
      "id": "OZuy0Ju0F8nP"
    },
    {
      "cell_type": "code",
      "source": [
        "all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "eaWyrqq-XA7E",
        "outputId": "425aab65-647f-4e08-e455-4196f3d8b8b2"
      },
      "id": "eaWyrqq-XA7E",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       published_year     ratio  referencies   cdindex  refer  fields  \\\n",
              "0                1946  1.000000            0  1.000000    0.0       2   \n",
              "1                1947  1.000000            9  0.000000    0.0       5   \n",
              "2                1947  0.968254           11 -0.071429    0.0       2   \n",
              "3                1948  1.000000            0  1.000000    1.0       2   \n",
              "4                1948  0.880000            4  1.000000    0.0       1   \n",
              "...               ...       ...          ...       ...    ...     ...   \n",
              "40936            2017  0.940000           38  0.000000    0.0       1   \n",
              "40937            2017  0.833333           44 -0.003106    0.0       2   \n",
              "40938            2017  0.845238           43  0.002105    0.0       1   \n",
              "40939            2017  0.936000           21 -0.013514    0.0       3   \n",
              "40940            2017  1.000000           11 -0.003906    0.0       4   \n",
              "\n",
              "       authors  times                          doi  \n",
              "0            1      2    10.1017/s0003598x00019207  \n",
              "1            4      5           10.4039/ent79161-9  \n",
              "2            2      2   10.1177/004051754701700802  \n",
              "3            1      4    10.1017/s0373463300034494  \n",
              "4            1      1            10.1090/qam/24251  \n",
              "...        ...    ...                          ...  \n",
              "40936       11     60  10.1152/ajprenal.00486.2016  \n",
              "40937        8    218           10.1039/c6tc04529d  \n",
              "40938        3      3   10.1108/ijchm-03-2015-0092  \n",
              "40939        8    331  10.1136/acupmed-2016-011139  \n",
              "40940       10    330    10.1017/s1478951517000864  \n",
              "\n",
              "[40941 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-760b7855-1f00-4c30-b1ee-6cf8de0fead4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>published_year</th>\n",
              "      <th>ratio</th>\n",
              "      <th>referencies</th>\n",
              "      <th>cdindex</th>\n",
              "      <th>refer</th>\n",
              "      <th>fields</th>\n",
              "      <th>authors</th>\n",
              "      <th>times</th>\n",
              "      <th>doi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1946</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>10.1017/s0003598x00019207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1947</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>10.4039/ent79161-9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1947</td>\n",
              "      <td>0.968254</td>\n",
              "      <td>11</td>\n",
              "      <td>-0.071429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>10.1177/004051754701700802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1948</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>10.1017/s0373463300034494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1948</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>4</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10.1090/qam/24251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40936</th>\n",
              "      <td>2017</td>\n",
              "      <td>0.940000</td>\n",
              "      <td>38</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>60</td>\n",
              "      <td>10.1152/ajprenal.00486.2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40937</th>\n",
              "      <td>2017</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>44</td>\n",
              "      <td>-0.003106</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>218</td>\n",
              "      <td>10.1039/c6tc04529d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40938</th>\n",
              "      <td>2017</td>\n",
              "      <td>0.845238</td>\n",
              "      <td>43</td>\n",
              "      <td>0.002105</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>10.1108/ijchm-03-2015-0092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40939</th>\n",
              "      <td>2017</td>\n",
              "      <td>0.936000</td>\n",
              "      <td>21</td>\n",
              "      <td>-0.013514</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>331</td>\n",
              "      <td>10.1136/acupmed-2016-011139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40940</th>\n",
              "      <td>2017</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11</td>\n",
              "      <td>-0.003906</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>330</td>\n",
              "      <td>10.1017/s1478951517000864</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40941 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-760b7855-1f00-4c30-b1ee-6cf8de0fead4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-760b7855-1f00-4c30-b1ee-6cf8de0fead4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-760b7855-1f00-4c30-b1ee-6cf8de0fead4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We will sellect features for creating **X** \n",
        "* We will define the y variable (target). \n",
        "* In addition, after many trials we conclused to remove **refer** variable because we have lower **MAE** (that is Mean Absolute Error).  "
      ],
      "metadata": {
        "id": "DaXGmko0sIwN"
      },
      "id": "DaXGmko0sIwN"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_data = all[['published_year','ratio','referencies',\n",
        "                    'times','fields','authors']] # remove refer \n",
        "X = feature_data\n",
        "y = all['cdindex']"
      ],
      "metadata": {
        "id": "TUlBMlXtHY1s"
      },
      "id": "TUlBMlXtHY1s",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We will split our data to training and testing sets (**20%**). "
      ],
      "metadata": {
        "id": "iBIh2ttusRNn"
      },
      "id": "iBIh2ttusRNn"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=13)"
      ],
      "metadata": {
        "id": "M-vlHZCGHVwB"
      },
      "id": "M-vlHZCGHVwB",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We will use TensorFlow's `Normalization()` layer to normalize our data."
      ],
      "metadata": {
        "id": "rUsHvYtmsZuV"
      },
      "id": "rUsHvYtmsZuV"
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = preprocessing.Normalization()\n",
        "normalizer.adapt(np.array(X_train))\n",
        "with np.printoptions(precision=2):\n",
        "    print(normalizer.mean)\n",
        "    print(normalizer.variance)\n",
        "    print(normalizer.count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW9wKXKmH1zO",
        "outputId": "d28064f3-9b49-40ec-d8e8-cb5dccc973b5"
      },
      "id": "SW9wKXKmH1zO",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[2.00e+03 9.17e-01 2.62e+01 1.21e+02 2.33e+00 4.05e+00]], shape=(1, 6), dtype=float32)\n",
            "tf.Tensor([[1.45e+02 5.74e-03 1.08e+03 1.64e+04 1.78e+00 2.96e+02]], shape=(1, 6), dtype=float32)\n",
            "<tf.Variable 'count:0' shape=() dtype=int64, numpy=32752>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Bearing in mind the algorithms we will use are probabilistic, we will initialize the random number generators with specific values to ensure reproducibility."
      ],
      "metadata": {
        "id": "XAkPetU2s-uR"
      },
      "id": "XAkPetU2s-uR"
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(13)\n",
        "tf.compat.v1.set_random_seed(13)\n",
        "rn.seed(12345)\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
      ],
      "metadata": {
        "id": "t42z6X50H3hr"
      },
      "id": "t42z6X50H3hr",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now with the `build_compile_model()` function we wiil create out neural network.\n",
        "\n",
        "* The neural network will consist of:\n",
        "\n",
        "    * An input layer.\n",
        "\n",
        "    * One hidden layer, of size **80**, connected to the input layer.\n",
        "\n",
        "    * One hidden layer, of size **50**, connected to the first hidden layer.\n",
        "\n",
        "    * One output layer, of size one.\n",
        "\n",
        "* Next, we will compile it using the **Adam** Optimizer."
      ],
      "metadata": {
        "id": "_1UpZABitJ-q"
      },
      "id": "_1UpZABitJ-q"
    },
    {
      "cell_type": "code",
      "source": [
        "## 2 \n",
        "def build_compile_model():\n",
        "    data_len = len(feature_data.columns)\n",
        "    \n",
        "    model = keras.Sequential([\n",
        "        normalizer,\n",
        "        layers.Dense(80, activation='relu'),         \n",
        "        layers.Dense(50, activation='relu'),  \n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='mean_absolute_error',\n",
        "                  optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = build_compile_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad8UyIn9hQ59",
        "outputId": "8166f0c3-d8d3-4b39-ae9f-136dd9dbfba1"
      },
      "id": "Ad8UyIn9hQ59",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " normalization_2 (Normalizat  (None, 6)                13        \n",
            " ion)                                                            \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 80)                560       \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 50)                4050      \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,674\n",
            "Trainable params: 4,661\n",
            "Non-trainable params: 13\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here we can observe that we have **4,674** parameters in total \n",
        "* In addition, the total rows of our dataframe are :: **40941**\n",
        "* This is a good because we don't want to the number of our parameters to be close with the number of the total rows of our dataframe (as it may be an indicator of overiffiting) \n",
        "* In addition, we will use some **callbacks**.\n",
        "    * For our train we will use the [ReduceLROnPlateau](https://www.bing.com/search?q=keras+ReduceLROnPlateau&cvid=88a5a61624544d59b98fd58b496d82cd&aqs=edge..69i57.1684j0j1&pglt=43&FORM=ANNTA1&PC=DCTS) callback, that reduces the learning rate of the model during the train when it is stack.\n",
        "    * We will also use the [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) callback that stops the train of the model when no improvement is noticed for many epochs.\n",
        "* Having done that, we are ready to train, i.e., fit the model.\n",
        "* Now, we are ready to train our regression Neural Network model (`fit`)"
      ],
      "metadata": {
        "id": "-Ykskwj1HHa4"
      },
      "id": "-Ykskwj1HHa4"
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(13)\n",
        "tf.compat.v1.set_random_seed(13)\n",
        "rn.seed(123)\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
        "\n",
        "num_epochs = 300  \n",
        "\n",
        "# callbacks \n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, mode='min', patience=15, min_lr=0.00005)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=num_epochs, \n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping, reduce_lr], # pass callbacks \n",
        "    verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-dwAFdwhT24",
        "outputId": "4ae15203-7903-4ebd-a900-7bccdf7ff436"
      },
      "id": "K-dwAFdwhT24",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "819/819 [==============================] - 4s 4ms/step - loss: 0.1108 - val_loss: 0.0657 - lr: 0.0010\n",
            "Epoch 2/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0589 - val_loss: 0.0654 - lr: 0.0010\n",
            "Epoch 3/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0513 - val_loss: 0.0545 - lr: 0.0010\n",
            "Epoch 4/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0478 - val_loss: 0.0535 - lr: 0.0010\n",
            "Epoch 5/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0466 - val_loss: 0.0481 - lr: 0.0010\n",
            "Epoch 6/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0441 - val_loss: 0.0447 - lr: 0.0010\n",
            "Epoch 7/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0437 - val_loss: 0.0464 - lr: 0.0010\n",
            "Epoch 8/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0428 - val_loss: 0.0431 - lr: 0.0010\n",
            "Epoch 9/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0421 - val_loss: 0.0452 - lr: 0.0010\n",
            "Epoch 10/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0414 - val_loss: 0.0475 - lr: 0.0010\n",
            "Epoch 11/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0414 - val_loss: 0.0463 - lr: 0.0010\n",
            "Epoch 12/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0409 - val_loss: 0.0444 - lr: 0.0010\n",
            "Epoch 13/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0404 - val_loss: 0.0388 - lr: 0.0010\n",
            "Epoch 14/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0396 - val_loss: 0.0415 - lr: 0.0010\n",
            "Epoch 15/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0389 - val_loss: 0.0418 - lr: 0.0010\n",
            "Epoch 16/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0386 - val_loss: 0.0445 - lr: 0.0010\n",
            "Epoch 17/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0382 - val_loss: 0.0452 - lr: 0.0010\n",
            "Epoch 18/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0381 - val_loss: 0.0391 - lr: 0.0010\n",
            "Epoch 19/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0380 - val_loss: 0.0395 - lr: 0.0010\n",
            "Epoch 20/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0377 - val_loss: 0.0391 - lr: 0.0010\n",
            "Epoch 21/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0377 - val_loss: 0.0372 - lr: 0.0010\n",
            "Epoch 22/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0373 - val_loss: 0.0379 - lr: 0.0010\n",
            "Epoch 23/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0377 - val_loss: 0.0369 - lr: 0.0010\n",
            "Epoch 24/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0363 - val_loss: 0.0382 - lr: 0.0010\n",
            "Epoch 25/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0369 - val_loss: 0.0377 - lr: 0.0010\n",
            "Epoch 26/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0369 - val_loss: 0.0379 - lr: 0.0010\n",
            "Epoch 27/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0365 - val_loss: 0.0390 - lr: 0.0010\n",
            "Epoch 28/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0362 - val_loss: 0.0370 - lr: 0.0010\n",
            "Epoch 29/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0357 - val_loss: 0.0409 - lr: 0.0010\n",
            "Epoch 30/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0361 - val_loss: 0.0376 - lr: 0.0010\n",
            "Epoch 31/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0356 - val_loss: 0.0398 - lr: 0.0010\n",
            "Epoch 32/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0354 - val_loss: 0.0359 - lr: 0.0010\n",
            "Epoch 33/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0357 - val_loss: 0.0392 - lr: 0.0010\n",
            "Epoch 34/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0355 - val_loss: 0.0360 - lr: 0.0010\n",
            "Epoch 35/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0351 - val_loss: 0.0354 - lr: 0.0010\n",
            "Epoch 36/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0354 - val_loss: 0.0367 - lr: 0.0010\n",
            "Epoch 37/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0349 - val_loss: 0.0357 - lr: 0.0010\n",
            "Epoch 38/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0348 - val_loss: 0.0361 - lr: 0.0010\n",
            "Epoch 39/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0346 - val_loss: 0.0359 - lr: 0.0010\n",
            "Epoch 40/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0346 - val_loss: 0.0373 - lr: 0.0010\n",
            "Epoch 41/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0349 - val_loss: 0.0351 - lr: 0.0010\n",
            "Epoch 42/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0345 - val_loss: 0.0354 - lr: 0.0010\n",
            "Epoch 43/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0344 - val_loss: 0.0367 - lr: 0.0010\n",
            "Epoch 44/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0344 - val_loss: 0.0359 - lr: 0.0010\n",
            "Epoch 45/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0342 - val_loss: 0.0372 - lr: 0.0010\n",
            "Epoch 46/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0342 - val_loss: 0.0381 - lr: 0.0010\n",
            "Epoch 47/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0344 - val_loss: 0.0362 - lr: 0.0010\n",
            "Epoch 48/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0342 - val_loss: 0.0355 - lr: 0.0010\n",
            "Epoch 49/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0344 - val_loss: 0.0356 - lr: 0.0010\n",
            "Epoch 50/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0340 - val_loss: 0.0366 - lr: 0.0010\n",
            "Epoch 51/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0341 - val_loss: 0.0362 - lr: 0.0010\n",
            "Epoch 52/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0340 - val_loss: 0.0354 - lr: 0.0010\n",
            "Epoch 53/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0341 - val_loss: 0.0373 - lr: 0.0010\n",
            "Epoch 54/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0340 - val_loss: 0.0354 - lr: 0.0010\n",
            "Epoch 55/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0340 - val_loss: 0.0364 - lr: 0.0010\n",
            "Epoch 56/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0341 - val_loss: 0.0347 - lr: 0.0010\n",
            "Epoch 57/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0343 - val_loss: 0.0351 - lr: 0.0010\n",
            "Epoch 58/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0340 - val_loss: 0.0384 - lr: 0.0010\n",
            "Epoch 59/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0338 - val_loss: 0.0349 - lr: 0.0010\n",
            "Epoch 60/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0339 - val_loss: 0.0371 - lr: 0.0010\n",
            "Epoch 61/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0340 - val_loss: 0.0355 - lr: 0.0010\n",
            "Epoch 62/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0337 - val_loss: 0.0338 - lr: 0.0010\n",
            "Epoch 63/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0338 - val_loss: 0.0346 - lr: 0.0010\n",
            "Epoch 64/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0339 - val_loss: 0.0343 - lr: 0.0010\n",
            "Epoch 65/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0341 - val_loss: 0.0344 - lr: 0.0010\n",
            "Epoch 66/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0341 - val_loss: 0.0347 - lr: 0.0010\n",
            "Epoch 67/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0335 - val_loss: 0.0362 - lr: 0.0010\n",
            "Epoch 68/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0338 - val_loss: 0.0346 - lr: 0.0010\n",
            "Epoch 69/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0337 - val_loss: 0.0349 - lr: 0.0010\n",
            "Epoch 70/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0335 - val_loss: 0.0349 - lr: 0.0010\n",
            "Epoch 71/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0338 - val_loss: 0.0359 - lr: 0.0010\n",
            "Epoch 72/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0336 - val_loss: 0.0368 - lr: 0.0010\n",
            "Epoch 73/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0337 - val_loss: 0.0359 - lr: 0.0010\n",
            "Epoch 74/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0337 - val_loss: 0.0355 - lr: 0.0010\n",
            "Epoch 75/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0336 - val_loss: 0.0351 - lr: 0.0010\n",
            "Epoch 76/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0337 - val_loss: 0.0349 - lr: 0.0010\n",
            "Epoch 77/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0338 - val_loss: 0.0353 - lr: 0.0010\n",
            "Epoch 78/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0331 - val_loss: 0.0354 - lr: 7.0000e-04\n",
            "Epoch 79/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0328 - val_loss: 0.0343 - lr: 7.0000e-04\n",
            "Epoch 80/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0329 - val_loss: 0.0339 - lr: 7.0000e-04\n",
            "Epoch 81/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0329 - val_loss: 0.0346 - lr: 7.0000e-04\n",
            "Epoch 82/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0330 - val_loss: 0.0367 - lr: 7.0000e-04\n",
            "Epoch 83/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0329 - val_loss: 0.0349 - lr: 7.0000e-04\n",
            "Epoch 84/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0330 - val_loss: 0.0333 - lr: 7.0000e-04\n",
            "Epoch 85/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0329 - val_loss: 0.0345 - lr: 7.0000e-04\n",
            "Epoch 86/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0327 - val_loss: 0.0344 - lr: 7.0000e-04\n",
            "Epoch 87/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0327 - val_loss: 0.0353 - lr: 7.0000e-04\n",
            "Epoch 88/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0329 - val_loss: 0.0357 - lr: 7.0000e-04\n",
            "Epoch 89/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0330 - val_loss: 0.0344 - lr: 7.0000e-04\n",
            "Epoch 90/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0328 - val_loss: 0.0346 - lr: 7.0000e-04\n",
            "Epoch 91/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0331 - val_loss: 0.0335 - lr: 7.0000e-04\n",
            "Epoch 92/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0330 - val_loss: 0.0337 - lr: 7.0000e-04\n",
            "Epoch 93/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0331 - val_loss: 0.0358 - lr: 7.0000e-04\n",
            "Epoch 94/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0328 - val_loss: 0.0341 - lr: 7.0000e-04\n",
            "Epoch 95/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0328 - val_loss: 0.0338 - lr: 7.0000e-04\n",
            "Epoch 96/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0330 - val_loss: 0.0343 - lr: 7.0000e-04\n",
            "Epoch 97/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0329 - val_loss: 0.0339 - lr: 7.0000e-04\n",
            "Epoch 98/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0329 - val_loss: 0.0339 - lr: 7.0000e-04\n",
            "Epoch 99/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0329 - val_loss: 0.0338 - lr: 7.0000e-04\n",
            "Epoch 100/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0323 - val_loss: 0.0340 - lr: 4.9000e-04\n",
            "Epoch 101/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0324 - val_loss: 0.0339 - lr: 4.9000e-04\n",
            "Epoch 102/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0325 - val_loss: 0.0327 - lr: 4.9000e-04\n",
            "Epoch 103/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0323 - val_loss: 0.0340 - lr: 4.9000e-04\n",
            "Epoch 104/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0323 - val_loss: 0.0338 - lr: 4.9000e-04\n",
            "Epoch 105/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0325 - val_loss: 0.0338 - lr: 4.9000e-04\n",
            "Epoch 106/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0322 - val_loss: 0.0342 - lr: 4.9000e-04\n",
            "Epoch 107/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0325 - val_loss: 0.0341 - lr: 4.9000e-04\n",
            "Epoch 108/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0324 - val_loss: 0.0329 - lr: 4.9000e-04\n",
            "Epoch 109/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0323 - val_loss: 0.0334 - lr: 4.9000e-04\n",
            "Epoch 110/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0325 - val_loss: 0.0339 - lr: 4.9000e-04\n",
            "Epoch 111/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0324 - val_loss: 0.0342 - lr: 4.9000e-04\n",
            "Epoch 112/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0326 - val_loss: 0.0331 - lr: 4.9000e-04\n",
            "Epoch 113/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0323 - val_loss: 0.0347 - lr: 4.9000e-04\n",
            "Epoch 114/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0325 - val_loss: 0.0336 - lr: 4.9000e-04\n",
            "Epoch 115/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0324 - val_loss: 0.0334 - lr: 4.9000e-04\n",
            "Epoch 116/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0322 - val_loss: 0.0339 - lr: 4.9000e-04\n",
            "Epoch 117/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0323 - val_loss: 0.0333 - lr: 4.9000e-04\n",
            "Epoch 118/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0321 - val_loss: 0.0329 - lr: 3.4300e-04\n",
            "Epoch 119/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0335 - lr: 3.4300e-04\n",
            "Epoch 120/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0329 - lr: 3.4300e-04\n",
            "Epoch 121/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0329 - lr: 3.4300e-04\n",
            "Epoch 122/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0321 - val_loss: 0.0327 - lr: 3.4300e-04\n",
            "Epoch 123/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0320 - val_loss: 0.0328 - lr: 3.4300e-04\n",
            "Epoch 124/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0321 - val_loss: 0.0328 - lr: 3.4300e-04\n",
            "Epoch 125/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0330 - lr: 3.4300e-04\n",
            "Epoch 126/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0320 - val_loss: 0.0333 - lr: 3.4300e-04\n",
            "Epoch 127/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0329 - lr: 3.4300e-04\n",
            "Epoch 128/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0319 - val_loss: 0.0332 - lr: 3.4300e-04\n",
            "Epoch 129/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0320 - val_loss: 0.0329 - lr: 3.4300e-04\n",
            "Epoch 130/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0320 - val_loss: 0.0330 - lr: 3.4300e-04\n",
            "Epoch 131/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0321 - val_loss: 0.0326 - lr: 3.4300e-04\n",
            "Epoch 132/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0327 - lr: 3.4300e-04\n",
            "Epoch 133/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0330 - lr: 3.4300e-04\n",
            "Epoch 134/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0319 - val_loss: 0.0332 - lr: 3.4300e-04\n",
            "Epoch 135/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0320 - val_loss: 0.0328 - lr: 3.4300e-04\n",
            "Epoch 136/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0319 - val_loss: 0.0328 - lr: 3.4300e-04\n",
            "Epoch 137/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0337 - lr: 3.4300e-04\n",
            "Epoch 138/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0340 - lr: 3.4300e-04\n",
            "Epoch 139/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0319 - val_loss: 0.0333 - lr: 3.4300e-04\n",
            "Epoch 140/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0330 - lr: 3.4300e-04\n",
            "Epoch 141/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0320 - val_loss: 0.0326 - lr: 3.4300e-04\n",
            "Epoch 142/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0333 - lr: 3.4300e-04\n",
            "Epoch 143/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0327 - lr: 3.4300e-04\n",
            "Epoch 144/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0319 - val_loss: 0.0329 - lr: 3.4300e-04\n",
            "Epoch 145/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0334 - lr: 3.4300e-04\n",
            "Epoch 146/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0319 - val_loss: 0.0340 - lr: 3.4300e-04\n",
            "Epoch 147/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0320 - val_loss: 0.0329 - lr: 2.4010e-04\n",
            "Epoch 148/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0317 - val_loss: 0.0331 - lr: 2.4010e-04\n",
            "Epoch 149/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0317 - val_loss: 0.0331 - lr: 2.4010e-04\n",
            "Epoch 150/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0317 - val_loss: 0.0328 - lr: 2.4010e-04\n",
            "Epoch 151/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0317 - val_loss: 0.0334 - lr: 2.4010e-04\n",
            "Epoch 152/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0318 - val_loss: 0.0328 - lr: 2.4010e-04\n",
            "Epoch 153/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0318 - val_loss: 0.0329 - lr: 2.4010e-04\n",
            "Epoch 154/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0319 - val_loss: 0.0330 - lr: 2.4010e-04\n",
            "Epoch 155/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0318 - val_loss: 0.0328 - lr: 2.4010e-04\n",
            "Epoch 156/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0318 - val_loss: 0.0330 - lr: 2.4010e-04\n",
            "Epoch 157/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0317 - val_loss: 0.0325 - lr: 2.4010e-04\n",
            "Epoch 158/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0317 - val_loss: 0.0329 - lr: 2.4010e-04\n",
            "Epoch 159/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0317 - val_loss: 0.0330 - lr: 2.4010e-04\n",
            "Epoch 160/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0318 - val_loss: 0.0325 - lr: 2.4010e-04\n",
            "Epoch 161/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0318 - val_loss: 0.0330 - lr: 2.4010e-04\n",
            "Epoch 162/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0316 - val_loss: 0.0327 - lr: 1.6807e-04\n",
            "Epoch 163/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0316 - val_loss: 0.0327 - lr: 1.6807e-04\n",
            "Epoch 164/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0316 - val_loss: 0.0325 - lr: 1.6807e-04\n",
            "Epoch 165/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0316 - val_loss: 0.0326 - lr: 1.6807e-04\n",
            "Epoch 166/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0316 - val_loss: 0.0325 - lr: 1.6807e-04\n",
            "Epoch 167/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0316 - val_loss: 0.0333 - lr: 1.6807e-04\n",
            "Epoch 168/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0316 - val_loss: 0.0326 - lr: 1.6807e-04\n",
            "Epoch 169/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0317 - val_loss: 0.0327 - lr: 1.6807e-04\n",
            "Epoch 170/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0316 - val_loss: 0.0327 - lr: 1.6807e-04\n",
            "Epoch 171/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0316 - val_loss: 0.0326 - lr: 1.6807e-04\n",
            "Epoch 172/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0316 - val_loss: 0.0328 - lr: 1.6807e-04\n",
            "Epoch 173/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0316 - val_loss: 0.0328 - lr: 1.6807e-04\n",
            "Epoch 174/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0315 - val_loss: 0.0328 - lr: 1.6807e-04\n",
            "Epoch 175/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0316 - val_loss: 0.0327 - lr: 1.6807e-04\n",
            "Epoch 176/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0316 - val_loss: 0.0326 - lr: 1.6807e-04\n",
            "Epoch 177/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0315 - val_loss: 0.0326 - lr: 1.1765e-04\n",
            "Epoch 178/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0315 - val_loss: 0.0324 - lr: 1.1765e-04\n",
            "Epoch 179/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0315 - val_loss: 0.0323 - lr: 1.1765e-04\n",
            "Epoch 180/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0315 - val_loss: 0.0324 - lr: 1.1765e-04\n",
            "Epoch 181/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0315 - val_loss: 0.0324 - lr: 1.1765e-04\n",
            "Epoch 182/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0314 - val_loss: 0.0325 - lr: 1.1765e-04\n",
            "Epoch 183/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0315 - val_loss: 0.0325 - lr: 1.1765e-04\n",
            "Epoch 184/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0315 - val_loss: 0.0326 - lr: 1.1765e-04\n",
            "Epoch 185/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0315 - val_loss: 0.0325 - lr: 1.1765e-04\n",
            "Epoch 186/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0315 - val_loss: 0.0324 - lr: 1.1765e-04\n",
            "Epoch 187/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0315 - val_loss: 0.0325 - lr: 1.1765e-04\n",
            "Epoch 188/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0315 - val_loss: 0.0325 - lr: 1.1765e-04\n",
            "Epoch 189/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0315 - val_loss: 0.0324 - lr: 1.1765e-04\n",
            "Epoch 190/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0315 - val_loss: 0.0325 - lr: 1.1765e-04\n",
            "Epoch 191/300\n",
            "819/819 [==============================] - 3s 3ms/step - loss: 0.0315 - val_loss: 0.0325 - lr: 1.1765e-04\n",
            "Epoch 192/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0315 - val_loss: 0.0326 - lr: 1.1765e-04\n",
            "Epoch 193/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0315 - val_loss: 0.0324 - lr: 1.1765e-04\n",
            "Epoch 194/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0314 - val_loss: 0.0325 - lr: 8.2354e-05\n",
            "Epoch 195/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0314 - val_loss: 0.0325 - lr: 8.2354e-05\n",
            "Epoch 196/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0314 - val_loss: 0.0325 - lr: 8.2354e-05\n",
            "Epoch 197/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0314 - val_loss: 0.0324 - lr: 8.2354e-05\n",
            "Epoch 198/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0314 - val_loss: 0.0325 - lr: 8.2354e-05\n",
            "Epoch 199/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0314 - val_loss: 0.0324 - lr: 8.2354e-05\n",
            "Epoch 200/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0314 - val_loss: 0.0325 - lr: 8.2354e-05\n",
            "Epoch 201/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0314 - val_loss: 0.0324 - lr: 8.2354e-05\n",
            "Epoch 202/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0314 - val_loss: 0.0326 - lr: 8.2354e-05\n",
            "Epoch 203/300\n",
            "819/819 [==============================] - 2s 2ms/step - loss: 0.0314 - val_loss: 0.0324 - lr: 8.2354e-05\n",
            "Epoch 204/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0314 - val_loss: 0.0325 - lr: 8.2354e-05\n",
            "Epoch 205/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0314 - val_loss: 0.0324 - lr: 8.2354e-05\n",
            "Epoch 206/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0314 - val_loss: 0.0324 - lr: 8.2354e-05\n",
            "Epoch 207/300\n",
            "819/819 [==============================] - 3s 4ms/step - loss: 0.0314 - val_loss: 0.0323 - lr: 8.2354e-05\n",
            "Epoch 208/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0314 - val_loss: 0.0325 - lr: 8.2354e-05\n",
            "Epoch 209/300\n",
            "819/819 [==============================] - 2s 3ms/step - loss: 0.0313 - val_loss: 0.0324 - lr: 5.7648e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Reading the summary we can observe the following ones: \n",
        "    * Both Loss an d Val Loss seems to decreases.\n",
        "    * The model stopped in **209** epochs thank to **Early Stopping** (it is a technique to avoid overfitting).\n",
        "    * In addition, was useful ReduceLROnPlateau callback since we can observe that it changes sometimes during the training process.   "
      ],
      "metadata": {
        "id": "pNL_L6XXJ_AE"
      },
      "id": "pNL_L6XXJ_AE"
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A15qkNR9hYLT",
        "outputId": "715c7ef1-015b-4d84-a4d6-589632111f4d"
      },
      "id": "A15qkNR9hYLT",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03307007625699043"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* We can observe that **MAE** is equal to **0.03307007625699043**. \n",
        "* Ouaou !! This is the **BEST** values of MAE that we managed bearing in mind the other models (Decision Tree Regressor, Random Forest, XBoost, LightGBM) that we trained in the main jupyter "
      ],
      "metadata": {
        "id": "EV8ztNVXJCWF"
      },
      "id": "EV8ztNVXJCWF"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(history):\n",
        "    plt.figure(figsize=(5, 3))\n",
        "    plt.plot(history.history['loss'], label='loss')\n",
        "    plt.plot(history.history['val_loss'], label='val_loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Error')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "plot_loss(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "bHbyQ5KZhZn4",
        "outputId": "1ffb8a47-c5f8-4e36-f1df-959c8244fe7c"
      },
      "id": "bHbyQ5KZhZn4",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x216 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAADQCAYAAABRLzm1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqr0lEQVR4nO3deXiU1dn48e89S/aFJEDCEgiryKKgAZcKRquCWqV1xRWtldZWu2ip2MWt2lZttW/f+rOlVV9qtaKoVSsVAY2gIrLIvu8kAZIACdkmmeX8/jgTCCGRZJLJEOb+XFeueeZZZu4zA/ecc57znEeMMSillGo9R6QDUEqpzkoTqFJKhUgTqFJKhUgTqFJKhUgTqFJKhUgTqFJKhcgV6QDaS9euXU1OTk6rjqmqqiIxMTE8AZ2gtMzRQcvcfpYtW1ZqjOnW1LaTJoHm5OSwdOnSVh2Tn59PXl5eeAI6QWmZo4OWuf2IyM7mtmkTXimlQqQJVCmlQqQJVCmlQnTS9IEqpZrm9XopKCjA4/FEOpSwSk1NZf369SEfHxcXR+/evXG73S0+JmoT6HP5W9m6rY4o62dXUaigoIDk5GRycnIQkUiHEzYVFRUkJyeHdKwxhv3791NQUEC/fv1afFzUNuEXbCphVYk/0mEoFXYej4eMjIyTOnm2lYiQkZHR6lp61CZQp0MI6Ex+Kkpo8jy+UD6jqE2gDk2gSnWYpKSkSIcQFlGbQJ2CJlClVJtEbwJ1ODSBKtXBjDFMnTqV4cOHM2LECGbOnAnAnj17GDduHCNHjmT48OEsXLgQv9/PbbfddnjfZ555JsLRHytqz8I7HRDQ25moKPPIu2tZV3SoXV9zaM8UHrpiWIv2ffPNN1mxYgUrV66ktLSU0aNHM27cOF555RXGjx/PL37xC/x+P9XV1axYsYLCwkLWrFkDQFlZWbvG3R6iuAaqfaBKdbRPPvmEG264AafTSWZmJueffz5Llixh9OjRvPjiizz88MOsXr2a5ORk+vfvz7Zt27jnnnt4//33SUlJiXT4x4jaGqhDNIGq6NPSmmJHGzduHAsWLOC9997jtttu49577+XWW29l5cqVzJkzh7/85S+89tprvPDCC5EO9SjRXQONdBBKRZmxY8cyc+ZM/H4/JSUlLFiwgDFjxrBz504yMzO58847+c53vsPy5cspLS0lEAhw9dVX89hjj7F8+fJIh3+MqK2BOh2CXzOoUh3qW9/6FosWLeL0009HRHjyySfJyspixowZPPXUU7jdbpKSkvjHP/5BYWEht99+O4GA/Y/629/+NsLRHyt6E6gI2oJXqmNUVlYCdrD6U089xVNPPXXU9smTJzN58uRjjjsRa50NRXcTXjOoUqoNojaB2iuRNIMqpUIXtQnUpTVQpVQbRW0CdYjg1wSqlGqDqE2gToegLXilVFtEdQLVJrxSqi3CmkBFZIKIbBSRLSIyrYnt40RkuYj4ROSaRtsmi8jm4N+x4xvaSBOoUqqtwpZARcQJPAtcCgwFbhCRoY122wXcBrzS6Nh04CHgLGAM8JCIpLVnfE7tA1XqhPRVc4fu2LGD4cOHd2A0Xy2cNdAxwBZjzDZjTB3wKjCx4Q7GmB3GmFVwzFWV44G5xpgDxpiDwFxgQnsG53DYgfRGO0KVUiEK55VIvYDdDZ4XYGuUoR7bq/FOIjIFmAKQmZlJfn5+i4PbvbMOgI/y83FE0e0OKisrW/U5nQyivcypqalUVFQAEPvRQziK17brewW6D6P2gkea3f7QQw/Rq1cvpkyZAsBvfvMbXC4XCxcupKysDK/Xy69+9Ssuv/zyw8fUx9tYZWUlgUCAiooKPB4PP/nJT/jyyy9xuVw89thj5OXlsX79eu666y68Xi+BQICXXnqJHj16MHnyZIqKivD7/fzsZz/j6quvPub1PR5Pq/6tdOpLOY0x04HpALm5uSavFbfYXBPYDFs2cd7Y84lxRc+5tPz8fFrzOZ0Mor3M69evP3K3SncMONv5v707hpivuBvmLbfcwo9//GPuu+8+AN5++23mzJnD1KlTSUlJobS0lLPPPpvrr7/+8H2Jmru7ZlJSEg6Hg+TkZKZPn05MTAxr165lw4YNXHzxxWzevJmXXnqJe++9l5tuuom6ujr8fj+zZ8+mT58+zJkzB4Dy8vIm3yMuLo5Ro0a1uOjhTKCFQHaD572D61p6bF6jY/PbJaogp8MmTb+eSVLR5NLfdfhbjho1iuLiYoqKiigpKSEtLY2srCx+8pOfsGDBAhwOB4WFhezbt4+srKwWv+4nn3zCPffcA8CQIUPIzs5m06ZNnHPOOTz++OMUFBRw1VVXMWjQIEaMGMF9993H/fffzze+8Q3Gjh3bLmULZ9VrCTBIRPqJSAwwCXinhcfOAS4RkbTgyaNLguvajTNYcr/2gSoVdtdeey2zZs1i5syZXH/99bz88suUlJSwbNkyVqxYQWZmZqtvKdycG2+8kXfeeYf4+Hguu+wyPvzwQwYPHszy5csZMWIEv/zlL3n00Ufb5b3ClkCNMT7gbmziWw+8ZoxZKyKPisiVACIyWkQKgGuBv4rI2uCxB4BfY5PwEuDR4Lp2U9/vqTVQpcLv+uuv59VXX2XWrFlce+21lJeX0717d9xuNx999BE7d+5s9WuOHTuWl19+GYBNmzZRUFDAKaecwrZt2+jfvz8//OEPmThxIqtWraKoqIiEhARuvvlmpk6d2m6zPIW1D9QYMxuY3Wjdgw2Wl2Cb500d+wIQtumnnQ6bQAOaQJUKu2HDhlFRUUGvXr3o0aMHN910E1dccQUjRowgNzeXIUOGtPo1v//973PXXXcxYsQIXC4Xzz33HLGxsbz22mu89NJLuN1usrKy+PnPf86SJUuYOnUqDocDt9vNc8891y7l6tQnkdrCFUyg2oRXqmOsXr368HLXrl1ZtGhRk/vVzx3alJycnMM3mYuLi+PFF188vK3+zP20adOYNu3o63bGjx/P+PHjQ469OdFz+rkRh0Ob8EqptonaGqhT+0CVOmGtXr2aW2655ah1sbGxLF68OEIRNS1qE6jWQJU6cY0YMYIVK1ZEOozjitomfH0NVGelV9FAL1k+vlA+o6hNoC6nTaA+rYGqk1xcXBz79+/XJPoVjDHs37+fuLi4Vh0XvU140WFMKjr07t2bgoICSkpKIh1KWHk8nlYnwIbi4uLo3bvJUZXNitoE6tRhTCpKuN1u+vXrF+kwwi4/P79V17G3h6htwuuVSEqptoraBOo6fCVShANRSnVaUZtA65vwPs2gSqkQRW0CrR8HqsOYlFKhitoEeuRKpAgHopTqtKI2gTrq5wPVk0hKqRBFbQJ16Yz0Sqk2itoEqjPSK6XaKmoTqF6JpJRqq6hNoE6djUkp1UaaQLUJr5QKkSZQrYEqpUIUvQlUr4VXSrVR1CZQvRJJKdVWUZtAtQaqlGqr6E2gDp2RXinVNlGfQHUcqFIqVFGfQHUYk1IqVFGbQPVKJKVUW0VtAnVpH6hSqo3CmkBFZIKIbBSRLSIyrYntsSIyM7h9sYjkBNe7RWSGiKwWkfUi8kB7x+bQgfRKqTYKWwIVESfwLHApMBS4QUSGNtrtDuCgMWYg8AzwRHD9tUCsMWYEcCbw3frk2l6cOg5UKdVG4ayBjgG2GGO2GWPqgFeBiY32mQjMCC7PAr4uIgIYIFFEXEA8UAccas/gdEZ6pVRbhTOB9gJ2N3heEFzX5D7GGB9QDmRgk2kVsAfYBfzeGHOgPYOrn5Fea6BKqVC5Ih1AM8YAfqAnkAYsFJF5xphtDXcSkSnAFIDMzEzy8/Nb/Ab1iXPL1m3kS0H7RN0JVFZWtupzOhlomaNDJMoczgRaCGQ3eN47uK6pfQqCzfVUYD9wI/C+McYLFIvIp0AucFQCNcZMB6YD5Obmmry8vBYHZ4yBObPJ7ptDXt7g1pSrU8vPz6c1n9PJQMscHSJR5nA24ZcAg0Skn4jEAJOAdxrt8w4wObh8DfChMcZgm+0XAohIInA2sKE9gxMRBB0HqpQKXdgSaLBP825gDrAeeM0Ys1ZEHhWRK4O7PQ9kiMgW4F6gfqjTs0CSiKzFJuIXjTGr2jtGh+iVSEqp0IW1D9QYMxuY3Wjdgw2WPdghS42Pq2xqfXtzio4DVUqFLmqvRIJgDVQTqFIqRMdNoCLiEJFzOyKYjiaaQJVSbXDcBGqMCWD7JE86DtFxoEqp0LW0CT9fRK4OXiV00tA+UKVUW7Q0gX4XeB2oE5FDIlIhIu16aWUkOEQ0gSqlQtais/DGmORwBxIJehJJKdUWLR7GFBy7OS74NN8Y85/whNRxBB0HqpQKXYua8CLyO+BHwLrg349E5LfhDKwjOESvRFJKha6lNdDLgJHBM/KIyAzgS6DdJzruSE7RGemVUqFrzUD6Lg2WU9s5jojQYUxKqbZoaQ30N8CXIvIRtutwHEeuW++09CSSUqotjptARcQBBLAzIo0Orr7fGLM3nIF1BBHRGemVUiE7bgI1xgRE5GfGmNc4djq6Ts0OpNcMqpQKTUv7QOeJyE9FJFtE0uv/whpZB7DT2UU6CqVUZ9XSPtDrg48/aLDOAP3bN5yOpcOYlFJt0dI+0GnGmJkdEE+H0pNISqm2aOlsTFM7IJYOp1ciKaXaIqr7QJ0OrYEqpUIX3X2g6GxMSqnQtXQ2pn7hDiQS9EokpVRbfGUTXkR+1mD52kbbfhOuoDrEqtc5x7cYv46kV0qF6Hh9oJMaLDeeOGRCO8fScYyBpS/wgOdpHqx4JNLRKKU6qeMlUGlmuannnYcITH6Xee4LOMu3FPzeSEeklOqEjpdATTPLTT3vXJwudrv62OXaisjGopTqlI53Eun04L2PBIhvcB8kAeLCGlkHqJFEu+Aph4ROPypLKdXBvjKBGmOcHRVIJNQ4EuyCpzyygSilOqXWTKh80vHUJ9DaTn+DUaVUBER5Aq1vwmsCVUq1XlgTqIhMEJGNIrJFRI6ZwV5EYkVkZnD7YhHJabDtNBFZJCJrRWS1iLR7n6tHtAaqlApd2BKoiDiBZ4FLgaHADSIytNFudwAHjTEDgWeAJ4LHuoB/At8zxgwD8oB2H2vk0T5QpVQbhLMGOgbYYozZZoypA14FJjbaZyIwI7g8C/i6iAhwCbDKGLMSwBiz3xjjb+8A6xzxdkGb8EqpEIQzgfYCdjd4XhBc1+Q+xhgfUA5kAIMBIyJzRGR5w0tK25NxuKg2sdqEV0qFpKWzMXU0F3Ae9iZ21cB8EVlmjJnfcCcRmQJMAcjMzCQ/P79Vb+LzeakggfLtG9jYymM7q8rKylZ/Tp2dljk6RKLM4UyghUB2g+e9g+ua2qcg2O+ZCuzH1lYXGGNKAURkNnAGcFQCNcZMB6YD5Obmmry8vFYF+NrGD6gggYFpCfRo5bGdVX5+Pq39nDo7LXN0iESZw9mEXwIMEpF+IhKDnZik8V093wEmB5evAT40xhhgDjBCRBKCifV8YF17BxjrhHKTQED7QJVSIQhbDdQY4xORu7HJ0Am8YIxZKyKPAkuNMe8AzwMvicgW4ADB2Z+MMQdF5GlsEjbAbGPMe+0dY0qMUGES8FWXEdPeL66UOumFtQ/UGDMbmN1o3YMNlj3AtY2PC277J3YoU9ikxAoVxGNq9oXzbZRSJ6movhKpvgYqddqEV0q1XlQn0OQY4RCJOOt0OjulVOtFdQJNiREOmXicgTrweiIdjlKqk4nqBBrvgpr6CUV0ML1SqpWiOoGKCCY2xT7RoUxKqVaK6gQKUJEQvK3Hyn9FNhClVKcT9Ql0f9ppzIu9CD55GgqXRzocpVQnEvUJNCMxlqfMrWACsHX+8Q9QSqmgqE+gXZNi2F4dg0nIgLLdxz9AKaWCoj6BZiTFUOcLEEjpDeWaQJVSLacJNDEWAE9ib62BKqVaJeoTaI9Ue6ulg+5MKC8AYyIckVKqs4j6BDqkhx0HutOfAb4aqCqNcERKqc4i6hNoemIMPVLjWFedaldoP6hSqoWiPoECDO2RwrLyJPtEE6hSqoU0gQJDe6aw+EDwmng9kaSUaiFNoNga6MFAAn53ktZAlVItpgkUGNYzFRAOxGVDycZjd9i5CHYt7vC4lFInNk2gQHZ6PGP6pfNZZQ/MnlVHD2UyBt6cAv+aBJ7yyAWplDrhaALFTms37dIhLKvLRmr2w741sHqW3Vi8Hsp3Qc0B+OSPEY1TKXVi0QQadEafNAKZI+yT1ybDG3fAwR2weY5d1+98WPwXqK2MWIxKqROLJtAGep4yhoAROLDVrtizEjZ9AFkj4Pz7wVsNm95v+uCKvXoVk1JRRhNoA2NOyWa7yTqyYls+7P4cBk+APudAck9Y88axB857BP5wChQs7bBYlVKRpwm0gdOzu7BSTuFATE/oPhS+fNnOEzpoPDgcMPwq2DwXasqOHLTpAzsZM0DppojErZSKDE2gDbidDub2vY8ra3/NrrjB4K+FhAzodYbdoX8eBLxQsuHIQWvegJhku1xR1OExK6UiRxNoI9Mmnkm3zB68sDV4s7mBF4PDaZdTs+1jecGRA3Z+CgMvhPh0OLSnY4NVSkWUJtBG+mYkMnPKOVR2GwXA3p4XHtmY2ss+1l+tdHCnXe57HqT0gkNaA1UqmmgCbUKMy8GPbp3ETc4nuerDDArLauyG2GSIS4XyQvt8xyf2Mec8SOmhTXiloowm0GZkpyfw8zsmUVHn55a/L2ZLcQVVtT7bjC8vsJd3fva/tunebQik9NQaqFJRJqwJVEQmiMhGEdkiItOa2B4rIjOD2xeLSE6j7X1EpFJEfhrOOJszrGcqL942mqLyGi56egFn/WY++6QrlO2EV2+wl3Z+4xl7hj65J1SVgK+udW9SW6GJV6lOKmwJVEScwLPApcBQ4AYRGdpotzuAg8aYgcAzwBONtj8N/DdcMbZEbk46b//gPH531QgGZyYxp8ANxeug5iBc9DAM+6bdMaWnfaxo5YmkeY/A3y/SQfhKdULhrIGOAbYYY7YZY+qAV4GJjfaZCMwILs8Cvi4iAiAi3wS2A2vDGGOLnJKVzKQxfZj53XPI7jfo8PpA9llHdkrpYR9bm0CLlsOhQvunlOpUXGF87V5Aw8k1C4CzmtvHGOMTkXIgQ0Q8wP3AxUCzzXcRmQJMAcjMzCQ/P79VAVZWVrb6mMwkexO6IpPO469t5NohOwBIqCpiDFD4/jPszxjNgfQzwf4WNM8Yztu7Fhew+d1nyNnxL9YM/yXlXRpX1NtPKGXu7LTM0SESZQ5nAm2Lh4FnjDGV8hVJyBgzHZgOkJuba/Ly8lr1Jvn5+bT2GHbGwvqnKUk7g/d2+Djt1IFMPjeHOF8FLLmHXkXv06vofRjyDZj0ctOvEQjAov+FHqfDxx4ABhX9G3xVjEqrAt8CO2nJZU+2LrYWCKnMnZyWOTpEoszhTKCFQHaD572D65rap0BEXEAqsB9bU71GRJ4EugABEfEYY/4cxnhbJr0/OFwMGzuRvFXd+O1/NzB9wTZ+d/VpXHTxr5HUXlD0pT1DX7wBug+xx/lq7aMrFlb+C+Y+CCm97TqHC6qDdwMtXgcFS6D2EFz6xPFrsU0xJrTjAn57ZdWwq8B5ov62KnXiCOf/kiXAIBHph02Uk4AbG+3zDjAZWARcA3xojDHA2PodRORhoPKESJ4AyZlw9xJcXXJ4YZTw2db9PD57PXf+YykJMYMZ0C2Ji7MncA9/purzF0iKj6NwwA1kfvILXPtWwdj7YMHv7WsdCl7RNPBi2BQ8V7Zr0ZF+1IPbbcJujboq+PvF9uqoSx5r3bFb5sGbd0JMIgy5vHXHKhWFwnYSyRjjA+4G5gDrgdeMMWtF5FERuTK42/PYPs8twL3AMUOdTkjp/cHhwOEQzhvUlbe+fy5PXD2C63KzSY5z8T9fHOJz/6kkLf8rfPo/BGZcgWvbfPB74f1gEb/+kH1MyoSBX7fLAy48+iRU4fKm3z8QsDXF+lptQ3MfhOK1diIUv7fp470eWPf2sWf+699vz6qWfQ5KRbmwttOMMbOB2Y3WPdhg2QNce5zXeDgswbWjOLeT60f3Ofx8b7mH0gVbYdkvmWdyuUiWsseks+Br77Jmw0a2myzuyerPWXF/tIPwR91sk3JVKWz90L6IM8Z2BYy4Bkq3QFrOkWb16tfhrSl2DGrut+1xe1dDfBos+Tv0GAl7VsD2j2HgRfaYymKI62KXl/wdPvgF3PzmkeQN9v3AvtbMm+1FAlf+qXUfht8HG9617xub3LJj9q2D1N4Ql9K691IqwvRKpDDISo1j+DfuhruXcsGDc6k++15+Hvge97+3k3mlXdi238MNLyznr5kPsevMafidcezt9jXofqp9gfQBkHUaFK2wkzr/ORfeuQeqD9hbjNRPn7cxOLnzW9+Fl74Jb3zHXmp60yyITYE1b9rtpVvgj6fB9PNJrNxpEzDY7XMfgg3v2dronhV2/e7P7boVL0PFvqYL6ffZWmzAf/T6T5+B12+Dt77XsrGtnnL42wUw91ct+3CVOoHomYJwEYGug3ACCRMeYmzSdrKKK/nl5adigN/P2cgflzj5/ZYyenbJZ+f+au69oA/3iIOS5KFUOZPJ2f1vZPbPAAMrX4EN/7EnlwDS+tka5raPbd+lOxH2b4a8n0NSNzt36fJ/QPYYWPkquGKgej8jVzwAvio7Bd/KV+x8p+n97YiAyn32xFZ936wJwIp/2n7bxpbPgPfuhW/9FU6fZNcVfQn5v4MufW2s/70fvv4gxCY1/zlt+gB8HpvMJ/wO3PFHb988194JYGijIcS7Poedn8HYe0P4cpRqH1oD7SDfPq8fv71qBImxLpJiXTx85TA+uf8Crjy9F12TYpkwLIunP9rFg77buX3T2Xx7/ZkU18XC7s+ZGT+JPUnDCHQfCpf9HsZNtY8+D7x2i52z9M75cO4P4ey77Bte8jj0OhPe/ZE9MTXhCbj9v9ivXGDCb2yCjE+DA9tgYbBWO+pm+5iUCX2/Bstm2D7XhoyBxX+1y8tm2NpoTZm9e2lid5iSD6O/A1/8Ff48Gr78p012r98OT/aH/zRIehvetaMQag/BxtlHv8/ch+Dla2DWHUcmcKn38RMw/5Gmb0OtVAfRGmgEZSTF8ofrTgfAGMPCzaXkb+zHLZlJDMpM5rdzsui989/MS/gm95deQXKFm7NcGYzOSaNmaw03ko7Xl0jNxU/SN+MUzIWPEOOyv4l+dyL+G94gpuAzPMk5xPW03QMrRj7O6H4pMOQKex+nYVfB8xfB0uchoSuMvBE+/h0MHm9vpPfGHbaGW74bcsZCWl9bsy3dCD1Hwa7P4OlToarYFuqWtyAhHS7/A5x2Pbx3H7z9A7strgtkDLTvdfoke6+pzfNg5E2wZT6seAWGfgsKl9na7Kd/hOHXwLp/w4ePwalXwKBLbI10+0L7ml/+Ey75dft/OX6v7Ss+7XpbHqWaoAn0BCEijBvcjXGDux1ed8adl1NScRH3Jcfy2db9/GfVHj7ZUsK89bZf8sv+M/iyqIpDbwXgrf/iEBjVJ42uSTEs3XGQ6jo/2ekJbNq3jety6xicmcyG4h5szzqVmsUFdEu/Bc8uP9363o17z3Jejp/ERTtdXHDxHziYdS6bq5MZF5uGe9a3kboKjDgwOHAYH4dSh5B83UvI/zvHzpM6+jt2zOuABvOnZo+xtdHC5fZGfadcCuK0fbr/vgsyh4O3yiap5Cz4+En44Jfw+bP2+JyxcNV0cMXZroSVr0DeA9DtFHtngNQ+tnvi1CvssSk94ezvQ5dsW6vd9Tm445GA107ysmOhrZW7E2w/8r41MODrkHu7fb/KElj2f3ZymNRsO2KicBlc/Xdby/aUQ2JG+P8xlO2CgO/4Q9j2rLJljU8Lf0yqSWJOkkkscnNzzdKlrbupW2e9WqO82ovfGNITYzhQVceCTSXs2F+Fxxvg8237qaz1cUpmMinxbraWVNInPYE3lhd85Tmd9MQYkmJd7DpQfdT6B1wv813Xe8wyF1DgT8OFn2XmFD72n8ZZ/btRur8Ud3wyZw3oyqEaHx9vKubcAV0pCs6hmpUaR6zLyfbSSvaWe/he3gBuzdoNr0+G6v1w/jS44AHbRP/jcNut0OccOO06GPpNW/urPmCb9xv/C5vm2ERdXgjXPA//vNoeE5Nkh3UFgkO3xAnGnuA6kDaS9NPG22Y/Yse51lUGJ8EuhJvfsAn239+zSRIgKcvWqk3AThqzYbY9oXfdDPtDULrZnrBL6t5+X6wxMOfntnvEHW9HSfQJXv3s99oyOYK9bqtn2TG7fb8Gk989+sIJbw0fL/yU8y+8qP1i6wTC9f9ZRJYZY3Kb3KYJNC88AZ1gdpRWEed2svyLRQw7YwwJMS6KKzwkxLhIT4whJc6FMfDFjgOsKzpESrybAd0SKSkppmLpq6zLmkhaciLnDMhgZHYaf/l4K3/J30puThq+gGHx9gNgYOygrizbdZB+XROJcznZe8hDTZ2ffl0T8fj8fLmrjLvyBnBFf8FZ8AU7ul9ESWUdyXEuzlz0A3rv+4iFF7zBiNHjSI13s2FvBSt3l+ENGIal1DLwzfG4jI/FPW9mdc63uXaAH8fW+Sw0pxPjcjDUu5bYmhJMzUF6nXYBzj3LYcFTtkbabxxkn23H2g66xA7henaMTZo1B+2JtCv/bJP7gW1w0SM2ae/+HFzxtvuidLOtMa8JjnS4/Pe2pth9mE1iVaX25N7q1+0oiuFX2xplSk/ofz746+DUKwGxd31d/Zq9+CFvmq0xz/6p7Yfeucgm9zNuhdF32mFlMYm2NrziZfjkGdvfXLkXRt1if4y69LHdIvMeocrEknjdX+z7VO6FrNPh4A77g5Q53HbLVBXbO876au2UitlnHUnQ9Ta+D/MfteXse+7R23y14K2xteWYJHDHdci/5eZoAm0DTaAtE64yV9b68PkDdEmIaXafOl+AqbNW8vaKpuc/7Ukppzp2Mj9wJiKQGOOistZ31D5JMYI4HNR6Dd5A4Ctr1Redmsl1Z2Qx8p1L6OYtYuNVH1CWNICNeyvI6ZoIQPnyt7hiw884OGwyH/X5IVUBJ+nFixm+6VnWXfA3dlY46ePfTv/eveia0Y3YufeTsukN6HOuTcQHt9s3c8XbmxCa4Am31Gw7xnfLXEjuYRNrfe24Sx/wHAJPma3FInYZ7PjZG1+3IyI+fAxWvWoTlDPWbvcHL54YeTOMfxyev8T2R2cMsgnXWw1pOdRVHiDGe6iJT0XsmNv629LEp9tjfB574jA5y96R1ltta8Sb59ptrljbf+3z2C4Vh8vWyGnwBSRl2R+ZtBw7FjlzqB3mdqgIxGETeXmB/Qv4oedI+xmVbLSfTVwXe/+xymL7Hp4y+6OR3MN+rgG/fXTF2haErw66DrY/fgnp7PriPfpkJNjX9XvtuGJj7CgQv8922fQ4zfbtJ3Zt/h9O409ME2jTNIFGxvbSKjbtqyA51kVKvJuuSbFU1nqJcTrplhzLhr2HWLCplIPVdQzolkjeKd3x+gOs2F3G+YO7kZ4Yc/h1PtxQTHpiDKNz0vEHDIVlNfgDhk37Knh89nqMgX6yhwFSxLzAmU3Gk0w1FSQctS7W5aDWF2hy//PTD5LW+xS6xQuZ1RvxHywgq3It8YmpHJAuFMQOoKLbGZR7/CQFDtEjMxN/RTH+g7tJCxzg7NI3qU7oxdqks9ne5VxSnB5y9rxPZpKbtd0up8Bjk2WXBDeOkvWML36e1d2+QbkjjYFVyynrcR5bXAPplhxL2b4CUpw15J1zLrFSi6twGZ6uI1j2xaecleWnhjj2ehPIqlyHSe5BetHHJO9bgj/vARypvfH/56fUxXeHIZeRsuczOyn4vrW2RiliR3hc8Sf49H9sTd0dZ2vMdVW2xpqQDg63TXYHd9rJxg9sa356xoQMm8ADATvvg/EHuyZcR34c6n+MYpNtDdffzCTl4jjygwUExIkjLsUm1KYPAAxc/jSMvqOZfZo4ShNo006EZNLRoqnMW4or8Xj9bFmzjFG5Z7F+TwWJsU76d0tidUEZ/gBcOKQ7hWU1rNhdxuDMJLJSbTO0S3wMa4vK6ZuRSFWtj1UF5ZRW1uJyCu+t2kNRWQ0Hqux/7N5pCaQnxrBxXwVJsS4E2F9VR0q8C7/fUFTuIcblIDMlFp/fkBDjZG+5h1i3E68vQK0vQHKci/3B10uOcxEIGKrq/KTGu6nx2r7cGKcDj9ePL3Dk/6zbKfgCpl3m4+7VJR4R8PkNXn8AEXA6BJfDgQgcqvGSEu+mS4Kb6jo/yXFuMIbkODcVHi+lwa6Y1Hg32e5yBkoRTqewh0z8fj+kZFEncXi8fur8AeJMHZm+Qg7G9sLvigevh9raGnwxKcQ4BJfTQawzQJKpwel04XQ5cTmdxIoPXLEYhC6eAjzuNBJ9B1i3p5Ku/U/HWV0MzjgSqcLtdBLnr0SMn/LkQfSs3cKgwcPo26dviz+Xr0qgehZenbQGdrcD+Es3O+ibkUjfjMTD23p1iT9qv/p9GxrVx57dTk+MITv9SA31prNa/p8PoMLjJcblINblPGabMTb5ORzCgao64t1O4mOcGGPw+s3hYWn1fP4A5TVeuiTEUFzhoUt8DHvKa1i8/QCCfR2nCBs3bmDY0FOJczvp1SWesmov3kCA6lo/pZW1eLx+an0BeqTGkRrvZntpFev2HMIpgstpk5cA/oDBFzAEAobkOBcHqr1UerwkxLioqPUhQHmNl8RYFwO6J1Hh8XGoxsuaQwks8uRQ5w+QEOPE6RDKCspwiBAf48DtdBAIGPwmjUCgGn+gCrdLSHC78AYOHU7i3uCjL7hc52+qVRCcyYwE2LIZETCmvIn9NgPwRKrQt08Tm0OgCVSpMEuOcze7TUQOn0Cv75qoXx/jOnZKQpfTQUaSbeL3SLU/Av27JdG/29E/APkVW8gb2autoZ9wjDH4Awa/OVLrNgYMhgULFpJ3/jji3E78AZt46/yBw618vzFU1fpIiW/++2gtTaBKqU5D6mvITWyLcwlxblvLdzoEp8N5+Hm9hj9S7UEv5VRKqRBpAlVKqRBpAlVKqRBpAlVKqRBpAlVKqRCdNAPpRaQE2NnKw7pyZBBZtNAyRwctc/vpa4zp1tSGkyaBhkJEljZ3hcHJSsscHbTMHUOb8EopFSJNoEopFaJoT6DTIx1ABGiZo4OWuQNEdR+oUkq1RbTXQJVSKmRRm0BFZIKIbBSRLSIyLdLxhIOI7BCR1SKyQkSWBteli8hcEdkcfOz0dyQTkRdEpFhE1jRY12Q5xfpT8HtfJSJnRC7y0DVT5odFpDD4fa8QkcsabHsgWOaNIjI+MlGHTkSyReQjEVknImtF5EfB9RH9nqMygYqIE3gWuBQYCtwgIkMjG1XYXGCMGdlgeMc0YL4xZhAwP/i8s/s/YEKjdc2V81JgUPBvCvBcB8XY3v6PY8sM8Ezw+x5pjJkNEPy3PQkYFjzm/wX/D3QmPuA+Y8xQ4GzgB8FyRfR7jsoECowBthhjthlj6oBXgYkRjqmjTARmBJdnAN+MXCjtwxizADjQaHVz5ZwI/MNYnwNdRKRHhwTajpopc3MmAq8aY2qNMduBLdj/A52GMWaPMWZ5cLkCWA/0IsLfc7Qm0F7A7gbPC4LrTjYG+EBElonIlOC6TGPMnuDyXiAzMqGFXXPlPNm/+7uDTdYXGnTPnFRlFpEcYBSwmAh/z9GaQKPFecaYM7DNmR+IyLiGG40dgnHSD8OIlnJim6kDgJHAHuAPEY0mDEQkCXgD+LEx5qjbjkbie47WBFoIZDd43ju47qRijCkMPhYDb2GbbfvqmzLBx+LIRRhWzZXzpP3ujTH7jDF+Y0wA+BtHmuknRZlFxI1Nni8bY94Mro7o9xytCXQJMEhE+olIDLaD/Z0Ix9SuRCRRRJLrl4FLgDXYck4O7jYZeDsyEYZdc+V8B7g1eJb2bKC8QROwU2vUx/ct7PcNtsyTRCRWRPphT6x80dHxtYWICPA8sN4Y83SDTZH9nu1dAaPvD7gM2ARsBX4R6XjCUL7+wMrg39r6MgIZ2LOVm4F5QHqkY22Hsv4L22T1Yvu67miunNibgz8b/N5XA7mRjr8dy/xSsEyrggmkR4P9fxEs80bg0kjHH0J5z8M2z1cBK4J/l0X6e9YrkZRSKkTR2oRXSqk20wSqlFIh0gSqlFIh0gSqlFIh0gSqlFIh0gSqOjUR8TeYfWhFe86sJSI5DWc7UqoxV6QDUKqNaowxIyMdhIpOWgNVJ6XgXKhPBudD/UJEBgbX54jIh8EJN+aLSJ/g+kwReUtEVgb/zg2+lFNE/hacg/IDEYmPWKHUCUcTqOrs4hs14a9vsK3cGDMC+DPwx+C6/wVmGGNOA14G/hRc/yfgY2PM6cAZ2Ku3wF72+KwxZhhQBlwd1tKoTkWvRFKdmohUGmOSmli/A7jQGLMtOAnFXmNMhoiUYi9x9AbX7zHGdBWREqC3Maa2wWvkAHONnawXEbkfcBtjHuuAoqlOQGug6mRmmllujdoGy370vIFqQBOoOpld3+BxUXD5M+zsWwA3AQuDy/OBu8De8kVEUjsqSNV56a+p6uziRWRFg+fvG2PqhzKlicgqbC3yhuC6e4AXRWQqUALcHlz/I2C6iNyBrWnehZ3tSKlmaR+oOikF+0BzjTGlkY5Fnby0Ca+UUiHSGqhSSoVIa6BKKRUiTaBKKRUiTaBKKRUiTaBKKRUiTaBKKRUiTaBKKRWi/w8LVAF3y0dmXAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* From the above plot we can observe the following ones: \n",
        "    * The flow of lines for both loss, val loss decreases during the epochs. \n",
        "    * The val_loss is quite close to loss line.\n",
        "* So, we can concluse that our model is **not** Overfitted (it may helps early stopping, too). "
      ],
      "metadata": {
        "id": "0c8KlSVXJ5zJ"
      },
      "id": "0c8KlSVXJ5zJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "* From the whole analysis our best model is the Neural Network Regressor with Mean Absolute Error equal to **0.03307007625699043**\n",
        "    * Then **Random Forest** is better with about MAE **~0.39**\n",
        "    * Then **Decision Tree** is the third betetr about MAE **~0.40**\n",
        "    * For both Random Forests and Decision Tree dominant important feature with big diffrence is the feature **refrencies**, whereas is **LightGBM** times, refrensies and published year are dominant but without big diffrence.\n",
        "    * Fianlly, **XBoost** has the lower MAE in comparison with other regressor models that we trained. \n",
        "* In addition, our **Decision Tree Classifier** plays supportive role for debugginh and having a general idea if cdindex will be high or medium, for low cdindexx the model is not predict well. \n",
        "* Furthermore, the idea fro removing the feature **refer** stemming from the fact than in trees **feature importances** we observed that **refer** has **low** values. \n",
        "* Finally, I would like to thank Professor **Panos Louridas** for the knowledge that offer to my colleague and me during the course **Applied Machine Learning**. \n"
      ],
      "metadata": {
        "id": "2w-8XYINLPiq"
      },
      "id": "2w-8XYINLPiq"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "5a12293b676897cb26b632b0c243cc182e9defefe71197b1ff91b175fc838aed"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}